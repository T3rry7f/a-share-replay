"""
Aè‚¡å†å²åˆ†æ—¶æ•°æ®ä¸‹è½½å™¨ - ä¼˜åŒ–ç‰ˆ
ä½œè€…: Antigravity
åŠŸèƒ½: é«˜æ•ˆä¸‹è½½å…¨å¸‚åœºè‚¡ç¥¨å†å²åˆ†æ—¶æˆäº¤æ•°æ®
"""

import pandas as pd
import os
from pytdx.hq import TdxHq_API
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from tqdm import tqdm
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download.log'),
        logging.StreamHandler()
    ]
)

class StockDataDownloader:
    """è‚¡ç¥¨å†å²åˆ†æ—¶æ•°æ®ä¸‹è½½å™¨"""
    
    # é€šè¾¾ä¿¡æœåŠ¡å™¨åˆ—è¡¨(æ”¯æŒä¸»å¤‡åˆ‡æ¢)
    TDX_SERVERS = [
        ('121.37.207.165', 7709),
        ('202.108.253.131', 7709),
        ('218.108.47.69', 7709),
    ]
    
    def __init__(self, stock_csv_path='data/eastmoney_all_stocks.csv'):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.stock_csv_path = stock_csv_path
        self.stocks_df = None
        self.load_stock_list()
        
    def load_stock_list(self):
        """åŠ è½½è‚¡ç¥¨åˆ—è¡¨"""
        try:
            self.stocks_df = pd.read_csv(self.stock_csv_path)
            logging.info(f"æˆåŠŸåŠ è½½ {len(self.stocks_df)} åªè‚¡ç¥¨ä¿¡æ¯")
            
            # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿è‚¡ç¥¨ä»£ç æ ¼å¼ç»Ÿä¸€
            self.stocks_df['stock_code'] = self.stocks_df['stock_code'].astype(str).str.zfill(6)
            
            # æ·»åŠ å¸‚åœºä»£ç åˆ—(tdxæ ¼å¼: ä¸Šæµ·=1, æ·±åœ³=0, åŒ—äº¤æ‰€=2)
            # ä¼˜å…ˆä½¿ç”¨ market_type åˆ—,å¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ exchange åˆ—
            if 'market_type' in self.stocks_df.columns:
                self.stocks_df['tdx_market'] = self.stocks_df['market_type']
            else:
                exchange_map = {'ä¸Šæµ·': 1, 'æ·±åœ³': 0, 'åŒ—äº¤æ‰€': 2}
                self.stocks_df['tdx_market'] = self.stocks_df['exchange'].map(exchange_map)
            
            # ç»Ÿè®¡å„å¸‚åœºè‚¡ç¥¨æ•°é‡
            market_counts = self.stocks_df.groupby('exchange').size()
            for market, count in market_counts.items():
                logging.info(f"  {market}: {count} åª")
            
        except Exception as e:
            logging.error(f"åŠ è½½è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    def test_connectivity(self):
        """æµ‹è¯•æœåŠ¡å™¨è¿é€šæ€§"""
        logging.info("æ­£åœ¨æµ‹è¯•é€šè¾¾ä¿¡æœåŠ¡å™¨è¿é€šæ€§...")
        available_servers = []
        
        for ip, port in self.TDX_SERVERS:
            api = TdxHq_API(heartbeat=True)
            try:
                with api.connect(ip, port, time_out=2):
                    # å°è¯•è·å–ä¸€ä¸ªç®€å•æ•°æ®æ¥éªŒè¯
                    if api.get_security_count(0) is not None:
                        logging.info(f"âœ… æœåŠ¡å™¨ {ip}:{port} è¿æ¥æ­£å¸¸")
                        available_servers.append((ip, port))
                    else:
                        logging.warning(f"âŒ æœåŠ¡å™¨ {ip}:{port} è¿æ¥æˆåŠŸä½†æ— å“åº”")
            except Exception as e:
                logging.error(f"âŒ æœåŠ¡å™¨ {ip}:{port} è¿æ¥å¤±è´¥: {e}")
        
        if not available_servers:
            logging.error("âš ï¸ è­¦å‘Š: æ‰€æœ‰é…ç½®çš„é€šè¾¾ä¿¡æœåŠ¡å™¨å‡æ— æ³•è¿æ¥ï¼ä¸‹è½½ä»»åŠ¡æå¯èƒ½å…¨éƒ¨å¤±è´¥ã€‚")
        else:
            logging.info(f"å¯ç”¨æœåŠ¡å™¨æ•°é‡: {len(available_servers)}")
            
        return available_servers

    def fetch_single_stock(self, stock_row, date_int, save_dir, retry_count=3):
        """
        ä¸‹è½½å•åªè‚¡ç¥¨çš„åˆ†æ—¶æ•°æ®
        """
        stock_code = stock_row['stock_code']
        market = stock_row['tdx_market']
        stock_name = stock_row['stock_name']
        
        # 0. å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è·³è¿‡
        file_path = os.path.join(save_dir, f"{stock_code}.parquet")
        if os.path.exists(file_path):
            return (stock_code, True, "æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡")
        
        last_error = None
        
        # å°è¯•å¤šä¸ªæœåŠ¡å™¨
        for server in self.TDX_SERVERS[:retry_count]:
            api = TdxHq_API()
            try:
                # å¢åŠ è¶…æ—¶è®¾ç½®
                if not api.connect(server[0], server[1], time_out=5):
                    last_error = f"æ— æ³•è¿æ¥æœåŠ¡å™¨ {server[0]}"
                    continue
                
                # åˆ†æ‰¹æŠ“å–æ•°æ®
                all_data = []
                start = 0
                batch_size = 2000
                
                while True:
                    data = api.get_history_transaction_data(
                        market, stock_code, start, batch_size, date_int
                    )
                    
                    if data is None:
                        # å¯èƒ½æ˜¯ç½‘ç»œä¸­æ–­
                        last_error = "è·å–æ•°æ®è¿”å›None(å¯èƒ½æ˜¯ç½‘ç»œä¸­æ–­)"
                        break
                        
                    if len(data) == 0:
                        break
                        
                    all_data.extend(data)
                    
                    # å¦‚æœè¿”å›æ•°æ®ä¸è¶³ä¸€æ‰¹,è¯´æ˜å·²ç»å…¨éƒ¨è·å–
                    if len(data) < batch_size:
                        break
                        
                    start += batch_size
                
                api.disconnect()
                
                # å¦‚æœä¸­é€”å‡ºé”™å¯¼è‡´ data is Noneï¼Œåˆ™è§†ä½œè¿™æ¬¡å°è¯•å¤±è´¥
                if data is None:
                    continue

                # ä¿å­˜æ•°æ®
                if all_data:
                    df = pd.DataFrame(all_data)
                    
                    # æ·»åŠ è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                    df['stock_code'] = stock_code
                    df['stock_name'] = stock_name
                    df['exchange'] = stock_row['exchange']
                    df['date'] = date_int
                    
                    # æ•°æ®ç±»å‹ä¼˜åŒ–(å‡å°æ–‡ä»¶å¤§å°)
                    if 'price' in df.columns:
                        df['price'] = df['price'].astype('float32')
                    if 'vol' in df.columns:
                        df['vol'] = df['vol'].astype('int32')
                    
                    # ä¿å­˜ä¸ºparquetæ ¼å¼(å‹ç¼©+å¿«é€Ÿè¯»å–)
                    file_path = os.path.join(save_dir, f"{stock_code}.parquet")
                    df.to_parquet(file_path, compression='gzip', index=False)
                    
                    return (stock_code, True, f"æˆåŠŸä¸‹è½½ {len(all_data)} æ¡æ•°æ®")
                else:
                    return (stock_code, False, f"å½“æ—¥æ— æ•°æ® (Exchange:{market})")
                    
            except Exception as e:
                last_error = str(e)
                try:
                    api.disconnect()
                except:
                    pass
                continue
        
        return (stock_code, False, f"å°è¯•å‡å¤±è´¥. é”™è¯¯: {last_error}")
    
    def download_all_stocks(self, date_int, max_workers=10, output_dir=None, max_retry=3, progress_callback=None):
        """
        ä¸‹è½½å…¨å¸‚åœºè‚¡ç¥¨æ•°æ®
        
        Args:
            date_int: æ—¥æœŸ(20251216æ ¼å¼)
            max_workers: çº¿ç¨‹æ•°
            output_dir: è¾“å‡ºç›®å½•
            max_retry: å¤±è´¥é‡è¯•æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° func(current, total)
        """
    def download_all_stocks(self, date_int, max_workers=10, output_dir=None, max_retry=3, progress_callback=None):
        """
        ä¸‹è½½å…¨å¸‚åœºè‚¡ç¥¨æ•°æ®
        """
        # 1. å…ˆæµ‹è¯•è¿é€šæ€§ï¼Œå¹¶åªä¿ç•™æœ‰æ•ˆæœåŠ¡å™¨
        valid_servers = self.test_connectivity()
        if valid_servers:
            logging.info(f"å°†ä»…ä½¿ç”¨ {len(valid_servers)} ä¸ªå¯ç”¨æœåŠ¡å™¨è¿›è¡Œä¸‹è½½")
            self.TDX_SERVERS = valid_servers
        else:
            logging.error("æ²¡æœ‰å¯ç”¨æœåŠ¡å™¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤åˆ—è¡¨ç»§ç»­ï¼ˆå¯èƒ½ä¼šå¾ˆæ…¢ï¼‰...")
        
        # åˆ›å»ºä¿å­˜ç›®å½• (æ–°æ ¼å¼: data/æ—¥æœŸ/tick)
        if output_dir is None:
            output_dir = f"data/{date_int}/tick"
        os.makedirs(output_dir, exist_ok=True)
        
        logging.info(f"å¼€å§‹ä¸‹è½½ {date_int} çš„æ•°æ®,å…± {len(self.stocks_df)} åªè‚¡ç¥¨")
        logging.info(f"æ•°æ®ä¿å­˜è‡³: {output_dir}")
        
        # ç¬¬ä¸€æ¬¡ä¸‹è½½
        success_count, failed_list = self._download_batch(
            self.stocks_df, date_int, output_dir, max_workers, "åˆæ¬¡ä¸‹è½½", progress_callback
        )
        
        # === è¯Šæ–­ä¿¡æ¯ï¼šæ‰“å°å‰å‡ ä¸ªå¤±è´¥çš„åŸå›  ===
        if failed_list:
            logging.info("-" * 30)
            logging.info("ğŸ›‘ å¤±è´¥è¯Šæ–­ (å‰5ä¸ª):")
            for code, reason in failed_list[:5]:
                logging.info(f"   {code}: {reason}")
            logging.info("-" * 30)
            
            # å¦‚æœåŸå› æ˜¯"æ— æ•°æ®", æç¤ºç”¨æˆ·
            if "å½“æ—¥æ— æ•°æ®" in failed_list[0][1]:
                logging.warning("âš ï¸ æç¤º: çœ‹èµ·æ¥æœåŠ¡å™¨æ²¡æœ‰è¿™ä¸€å¤©çš„æ•°æ®ã€‚")
                logging.warning("   å¦‚æœæ˜¯ä¸‹è½½ã€ä»Šå¤©ã€‘çš„æ•°æ®ï¼Œé€šå¸¸éœ€è¦ç­‰åˆ°æ™šä¸Š(18:00å)æœåŠ¡å™¨å½’æ¡£åæ‰èƒ½ä¸‹è½½å†å²åˆ†æ—¶ã€‚")
        # ==================================
        
        # å¤±è´¥é‡è¯• (å¦‚æœæ˜¯å› ä¸ºæ— æ•°æ®ï¼Œé‡è¯•ä¹Ÿæ²¡ç”¨ï¼Œè¿™é‡Œç®€å•åˆ¤æ–­ä¸€ä¸‹)
        # å¦‚æœå¤§é‡å¤±è´¥ä¸”åŸå› æ˜¯æ— æ•°æ®ï¼Œè·³è¿‡é‡è¯•
        if len(failed_list) > len(self.stocks_df) * 0.9 and "å½“æ—¥æ— æ•°æ®" in failed_list[0][1]:
             logging.warning("ç»å¤§å¤šæ•°è‚¡ç¥¨æ— æ•°æ®ï¼Œè·³è¿‡é‡è¯•ã€‚")
        else:
            retry_round = 1
            while failed_list and retry_round <= max_retry:
                logging.info(f"ç¬¬ {retry_round} æ¬¡é‡è¯•,å‰©ä½™å¤±è´¥è‚¡ç¥¨: {len(failed_list)} åª")
                
                # ä»å¤±è´¥åˆ—è¡¨ä¸­è·å–è‚¡ç¥¨ä»£ç 
                failed_codes = [code for code, _ in failed_list]
                retry_df = self.stocks_df[self.stocks_df['stock_code'].isin(failed_codes)]
                
                retry_success, retry_failed = self._download_batch(
                    retry_df, date_int, output_dir, max_workers, f"ç¬¬{retry_round}æ¬¡é‡è¯•", None
                )
                
                success_count += retry_success
                failed_list = retry_failed
                retry_round += 1
        
        # ========================================
        # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šè‡ªåŠ¨åˆå¹¶ä¸ºå•æ–‡ä»¶æ ¼å¼
        # ========================================
        if success_count > 0:
            self._merge_to_single_file(date_int, output_dir)
        
        # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
        self._generate_report(date_int, output_dir, success_count, failed_list)
    
    def _download_batch(self, stocks_df, date_int, output_dir, max_workers, batch_name, progress_callback=None):
        """
        æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®
        """
        success_count = 0
        failed_list = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(self.fetch_single_stock, row, date_int, output_dir): row['stock_code']
                for _, row in stocks_df.iterrows()
            }
            
            total_tasks = len(futures)
            completed_tasks = 0
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
            with tqdm(total=total_tasks, desc=batch_name, ncols=100) as pbar:
                for future in as_completed(futures):
                    stock_code, success, message = future.result()
                    
                    if success:
                        success_count += 1
                    else:
                        failed_list.append((stock_code, message))
                    
                    pbar.update(1)
                    pbar.set_postfix({'æˆåŠŸ': success_count, 'å¤±è´¥': len(failed_list)})
                    
                    # è°ƒç”¨å¤–éƒ¨å›è°ƒ
                    completed_tasks += 1
                    if progress_callback:
                        try:
                            progress_callback(completed_tasks, total_tasks)
                        except:
                            pass
        
        return success_count, failed_list
    
    def _merge_to_single_file(self, date_int, output_dir):
        """
        å°†ä¸‹è½½çš„åˆ†æ•£parquetæ–‡ä»¶åˆå¹¶ä¸ºå•ä¸ªä¼˜åŒ–æ ¼å¼çš„æ–‡ä»¶
        
        Args:
            date_int: æ—¥æœŸ
            output_dir: è¾“å‡ºç›®å½•
        """
        import time
        from pathlib import Path
        import shutil
        
        logging.info("="*50)
        logging.info("âš¡ å¼€å§‹åˆå¹¶æ•°æ®æ–‡ä»¶...")
        start_time = time.time()
        
        # æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶ï¼ˆæ’é™¤å·²å­˜åœ¨çš„tick_data.parquetï¼‰
        tick_dir = Path(output_dir)
        parquet_files = [f for f in tick_dir.glob("*.parquet") if f.name != "tick_data.parquet"]
        
        if not parquet_files:
            logging.warning("æœªæ‰¾åˆ°éœ€è¦åˆå¹¶çš„æ–‡ä»¶")
            return
        
        logging.info(f"   å‘ç° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        # è¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶
        all_dataframes = []
        for file_path in parquet_files:
            try:
                df = pd.read_parquet(file_path)
                
                # ç¡®ä¿æœ‰stock_codeåˆ—
                if 'stock_code' not in df.columns:
                    df['stock_code'] = file_path.stem
                
                all_dataframes.append(df)
            except Exception as e:
                logging.warning(f"   è¯»å– {file_path.name} å¤±è´¥: {e}")
        
        if not all_dataframes:
            logging.error("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®æ–‡ä»¶")
            return
        
        # åˆå¹¶æ‰€æœ‰DataFrame
        logging.info("   æ­£åœ¨åˆå¹¶æ•°æ®...")
        combined_df = pd.concat(all_dataframes, ignore_index=True)
        
        # ç¡®ä¿datetimeåˆ—å­˜åœ¨
        if 'datetime' not in combined_df.columns:
            if 'time' in combined_df.columns and 'date' in combined_df.columns:
                combined_df['datetime'] = pd.to_datetime(
                    combined_df['date'].astype(str) + ' ' + combined_df['time'].astype(str)
                )
        
        # æ•°æ®ç±»å‹ä¼˜åŒ–
        if 'stock_code' in combined_df.columns:
            combined_df['stock_code'] = combined_df['stock_code'].astype('category')
        if 'stock_name' in combined_df.columns:
            combined_df['stock_name'] = combined_df['stock_name'].astype('category')
        
        # æ’åºï¼ˆæŒ‰è‚¡ç¥¨ä»£ç å’Œæ—¶é—´ï¼‰
        if 'datetime' in combined_df.columns:
            logging.info("   æ­£åœ¨æ’åº...")
            combined_df = combined_df.sort_values(['stock_code', 'datetime']).reset_index(drop=True)
        
        # ä¿å­˜ä¸ºå•ä¸ªä¼˜åŒ–æ–‡ä»¶ï¼ˆä¿å­˜åˆ°æ—¥æœŸç›®å½•ï¼Œæ–‡ä»¶åå›ºå®šä¸º tick_data.parquetï¼‰
        merged_file = tick_dir.parent / "tick_data.parquet"
        logging.info(f"   æ­£åœ¨ä¿å­˜åˆ° {merged_file}...")
        
        combined_df.to_parquet(
            merged_file,
            engine='pyarrow',
            compression='snappy',
            index=False
        )
        
        file_size_mb = merged_file.stat().st_size / 1024 / 1024
        elapsed = time.time() - start_time
        
        logging.info(f"âœ… åˆå¹¶å®Œæˆ!")
        logging.info(f"   æ–‡ä»¶: {merged_file}")
        logging.info(f"   å¤§å°: {file_size_mb:.2f} MB")
        logging.info(f"   æ€»è¡Œæ•°: {len(combined_df):,}")
        logging.info(f"   è‚¡ç¥¨æ•°: {combined_df['stock_code'].nunique():,}")
        logging.info(f"   è€—æ—¶: {elapsed:.2f} ç§’")
        
        # è¯¢é—®æ˜¯å¦åˆ é™¤åŸå§‹åˆ†æ•£æ–‡ä»¶
        logging.info("-"*50)
        logging.info("ğŸ’¡ æç¤º: åˆ†æ•£æ–‡ä»¶å·²åˆå¹¶ä¸ºå•æ–‡ä»¶ï¼Œæ˜¯å¦åˆ é™¤åŸå§‹åˆ†æ•£æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´ï¼Ÿ")
        logging.info(f"   åˆ†æ•£æ–‡ä»¶ç›®å½•: {tick_dir}")
        logging.info(f"   åˆå¹¶æ–‡ä»¶: {merged_file}")
        logging.info("   (æ‚¨å¯ä»¥æ‰‹åŠ¨åˆ é™¤ tick_* ç›®å½•ä»¥èŠ‚çœç©ºé—´)")
        logging.info("-"*50)
        
        return merged_file
        
    def _generate_report(self, date_int, output_dir, success_count, failed_list):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
        total = len(self.stocks_df)
        
        logging.info("="*50)
        logging.info(f"ä¸‹è½½å®Œæˆ!")
        logging.info(f"æ—¥æœŸ: {date_int}")
        logging.info(f"æ€»è®¡: {total} åªè‚¡ç¥¨")
        logging.info(f"æˆåŠŸ: {success_count} ({success_count/total*100:.1f}%)")
        logging.info(f"å¤±è´¥: {len(failed_list)} ({len(failed_list)/total*100:.1f}%)")
        logging.info("="*50)
        
        # ä¿å­˜å¤±è´¥åˆ—è¡¨
        if failed_list:
            failed_df = pd.DataFrame(failed_list, columns=['stock_code', 'reason'])
            failed_path = os.path.join(output_dir, 'failed_stocks.csv')
            failed_df.to_csv(failed_path, index=False)
            logging.info(f"å¤±è´¥åˆ—è¡¨å·²ä¿å­˜è‡³: {failed_path}")
    
    def download_date_range(self, start_date, end_date, max_workers=10, max_retry=3):
        """
        æ‰¹é‡ä¸‹è½½æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸå­—ç¬¦ä¸² '20251201'
            end_date: ç»“æŸæ—¥æœŸå­—ç¬¦ä¸² '20251220'
            max_workers: çº¿ç¨‹æ•°
            max_retry: æ¯æ—¥å¤±è´¥é‡è¯•æ¬¡æ•°
        """
        from datetime import datetime, timedelta
        
        start = datetime.strptime(start_date, '%Y%m%d')
        end = datetime.strptime(end_date, '%Y%m%d')
        
        current = start
        while current <= end:
            # è·³è¿‡å‘¨æœ«
            if current.weekday() < 5:  # 0-4æ˜¯å‘¨ä¸€åˆ°å‘¨äº”
                date_int = int(current.strftime('%Y%m%d'))
                logging.info(f"\n{'='*60}")
                logging.info(f"å¤„ç†æ—¥æœŸ: {current.strftime('%Y-%m-%d')} ({current.strftime('%A')})")
                logging.info(f"{'='*60}")
                
                self.download_all_stocks(date_int, max_workers, max_retry=max_retry)
            
            current += timedelta(days=1)


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader = StockDataDownloader()
    
    # ç¤ºä¾‹1: ä¸‹è½½å•ä¸ªäº¤æ˜“æ—¥æ•°æ®
    # downloader.download_all_stocks(20251216, max_workers=10)
    
    # ç¤ºä¾‹2: æ‰¹é‡ä¸‹è½½å¤šæ—¥æ•°æ®
    downloader.download_date_range('20251210', '20251220', max_workers=15)


if __name__ == "__main__":
    main()
