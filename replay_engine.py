"""
Aè‚¡å†å²å¤ç›˜ç³»ç»Ÿ - æ ¸å¿ƒå¼•æ“
åŠŸèƒ½: ç§’çº§åˆ†æ—¶æ•°æ®å›æ”¾ã€å®æ—¶æ’è¡Œæ¦œè®¡ç®—å±•ç¤º
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, time
import logging
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from config import SECTOR_MAPPING_CONFIG

logging.basicConfig(level=logging.INFO)


class ReplayEngine:
    """å¤ç›˜å¼•æ“ - è´Ÿè´£æ•°æ®å›æ”¾å’Œè®¡ç®—"""
    
    def __init__(self, data_dir: str):
        """
        åˆå§‹åŒ–å¤ç›˜å¼•æ“
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„(å¦‚ data/tick_20251216)
        """
        self.data_dir = Path(data_dir)
        self.all_data = {}  # {stock_code: DataFrame}
        self.current_time = None  # å½“å‰å›æ”¾æ—¶é—´
        self.start_time = time(9, 30)  # å¼€ç›˜æ—¶é—´
        self.end_time = time(15, 0)   # æ”¶ç›˜æ—¶é—´
        
        # å¤šç»´åº¦æ¿å—æ˜ å°„
        self.industry_map = {}  # {stock_code: [industry_list]}
        self.concept_map = {}   # {stock_code: [concept_list]}
        self.region_map = {}    # {stock_code: [region_list]}
        self.stock_name_map = {}  # {stock_code: stock_name}
        self.pre_close_map = {}  # {stock_code: pre_close} çœŸå®æ˜¨æ”¶ä»·
        
        # å®æ—¶ç¼“å­˜
        self.fast_data_cache = {} # {code: (times, prices, vols, pre_close)} çº¯NumPyæé€Ÿç¼“å­˜
        self.stock_cache = {}  # è‚¡ç¥¨å®æ—¶æ•°æ®ç¼“å­˜
        self.sector_cache = {}  # æ¿å—å®æ—¶æ•°æ®ç¼“å­˜
        
        # å¿«ç…§ç¼“å­˜ï¼ˆLRUç¼“å­˜ï¼Œæœ€å¤šä¿å­˜100ä¸ªæ—¶é—´ç‚¹çš„å¿«ç…§ï¼‰
        self.snapshot_cache = {}  # {time_key: snapshot_data}
        self.snapshot_cache_size = 100
        self.snapshot_cache_order = []  # LRU é¡ºåºè®°å½•
        
        # åŠ è½½è‚¡ç¥¨ä¿¡æ¯
        self.load_stock_names()
        self.load_sector_mappings()
        self.load_pre_close_prices()
        
    def load_all_data(self, progress_callback=None):
        """
        åŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®åˆ°å†…å­˜
        
        æ³¨æ„: è¿™ä¼šå ç”¨å¤§é‡å†…å­˜,å»ºè®®åªåœ¨å†…å­˜å……è¶³æ—¶ä½¿ç”¨
        æˆ–è€…é‡‡ç”¨æŒ‰éœ€åŠ è½½ç­–ç•¥
        """
        logging.info("å¼€å§‹åŠ è½½æ•°æ®...")
        
        parquet_files = list(self.data_dir.glob("*.parquet"))
        total = len(parquet_files)
        
        for idx, file_path in enumerate(parquet_files):
            stock_code = file_path.stem
            try:
                df = pd.read_parquet(file_path)
                
                # æ•°æ®é¢„å¤„ç†
                df = self._preprocess_tick_data(df)
                
                self.all_data[stock_code] = df
                
                if progress_callback and idx % 100 == 0:
                    progress_callback(idx, total)
                    
            except Exception as e:
                logging.warning(f"åŠ è½½ {stock_code} å¤±è´¥: {e}")
        
        logging.info(f"æ•°æ®åŠ è½½å®Œæˆ,å…± {len(self.all_data)} åªè‚¡ç¥¨")
    
    def detect_data_time_range(self):
        """
        æ£€æµ‹å·²åŠ è½½æ•°æ®çš„å®é™…æ—¶é—´èŒƒå›´
        """
        if not self.all_data:
            return
        
        min_time = None
        max_time = None
        
        # é‡‡æ ·æ£€æŸ¥ï¼ˆé¿å…éå†æ‰€æœ‰è‚¡ç¥¨ï¼‰
        sample_size = min(100, len(self.all_data))
        sample_codes = list(self.all_data.keys())[:sample_size]
        
        for code in sample_codes:
            df = self.all_data[code]
            if not df.empty and 'datetime' in df.columns:
                stock_min = df['datetime'].min()
                stock_max = df['datetime'].max()
                
                if min_time is None or stock_min < min_time:
                    min_time = stock_min
                if max_time is None or stock_max > max_time:
                    max_time = stock_max
        
        
        # é™åˆ¶ç»“æŸæ—¶é—´ä¸º15:00ï¼ˆAè‚¡æ”¶ç›˜æ—¶é—´ï¼‰
        # æ•°æ®å¯èƒ½åŒ…å«å°¾ç›˜é›†åˆç«ä»·ï¼Œä½†æ˜¾ç¤ºæ—¶æˆªæ–­åˆ°15:00
        if max_time is not None:
            market_close = max_time.replace(hour=15, minute=0, second=0, microsecond=0)
            if max_time.time() > time(15, 0):
                max_time = market_close
                logging.info(f"æ£€æµ‹åˆ°æ•°æ®è¶…è¿‡15:00ï¼Œè‡ªåŠ¨æˆªæ–­åˆ°15:00")
        
        self.data_start_time = min_time
        self.data_end_time = max_time
        
        logging.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {min_time.strftime('%H:%M:%S') if min_time else 'N/A'} - {max_time.strftime('%H:%M:%S') if max_time else 'N/A'}")
    
    def _preprocess_tick_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¢„å¤„ç†tickæ•°æ®
        
        Args:
            df: åŸå§‹tickæ•°æ®
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        if df.empty:
            return df
        
        # è§£ææ—¶é—´å­—æ®µ
        if 'time' in df.columns and 'date' in df.columns:
            # ç»„åˆdateå’Œtimeåˆ›å»ºå®Œæ•´çš„datetime
            date_col = df['date'].astype(str)
            time_col = df['time'].astype(str)
            df['datetime'] = pd.to_datetime(date_col + ' ' + time_col)
        elif 'time' in df.columns:
            # å¦‚æœåªæœ‰time,å‡è®¾æ˜¯ä»Šå¤©
            from datetime import date as dt_date
            today = dt_date.today().strftime('%Y%m%d')
            df['datetime'] = pd.to_datetime(today + ' ' + df['time'].astype(str))
        else:
            logging.warning("æ•°æ®ä¸­æ²¡æœ‰æ—¶é—´å­—æ®µ")
            return df
        
        # æ’åº
        df = df.sort_values('datetime').reset_index(drop=True)
        
        # --- åˆ†ç§’å¹³æ»‘å¤„ç† (Plan A) ---
        # å¦‚æœæ•°æ®åªæœ‰åˆ†é’Ÿç²¾åº¦ï¼Œå°†åŒä¸€åˆ†é’Ÿå†…çš„å¤šç¬”æˆäº¤å‡åŒ€åˆ†å¸ƒåœ¨ 60 ç§’å†…
        if len(df) > 1:
            df['_min_group'] = df['datetime'].dt.floor('min')
            df['_cum_count'] = df.groupby('_min_group').cumcount()
            df['_total_in_min'] = df.groupby('_min_group')['datetime'].transform('count')
            
            # åªæœ‰å½“ç§’æ•°ä¸º 0 æ—¶æ‰å°è¯•å¹³æ»‘ï¼ˆé¿å…ç ´ååŸæœ¬å°±æœ‰ç§’æ•°çš„æ•°æ®ï¼‰
            # æ£€æŸ¥ç¬¬ä¸€ç¬”æ˜¯å¦æœ‰ç§’æ•°
            if df['datetime'].iloc[0].second == 0:
                df['datetime'] = df['_min_group'] + pd.to_timedelta(
                    (df['_cum_count'] * 60 / df['_total_in_min']).astype(int), unit='s'
                )
            
            df.drop(columns=['_min_group', '_cum_count', '_total_in_min'], inplace=True)
            # å¹³æ»‘åé‡æ–°æ’åºä»¥é˜²ä¸‡ä¸€
            df = df.sort_values('datetime').reset_index(drop=True)
        
        # è®¡ç®—ç´¯è®¡æˆäº¤é‡
        if 'vol' in df.columns:
            df['cum_volume'] = df['vol'].cumsum()
        
        # è®¾ç½®æ˜¨æ”¶ä»·
        if 'price' in df.columns and len(df) > 0:
            if 'pre_close' not in df.columns:
                # è·å–è‚¡ç¥¨ä»£ç 
                if 'stock_code' in df.columns:
                    stock_code = str(df['stock_code'].iloc[0]).zfill(6)
                    # ä¼˜å…ˆä½¿ç”¨çœŸå®æ˜¨æ”¶ä»·
                    if stock_code in self.pre_close_map:
                        df['pre_close'] = self.pre_close_map[stock_code]
                    else:
                        # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç¬¬ä¸€ç¬”ä»·æ ¼
                        df['pre_close'] = df['price'].iloc[0]
                        logging.warning(f"{stock_code}: æœªæ‰¾åˆ°æ˜¨æ”¶ä»·ï¼Œä½¿ç”¨ç¬¬ä¸€ç¬”ä»·æ ¼ {df['price'].iloc[0]:.2f}")
                else:
                    # å¦‚æœæ²¡æœ‰ stock_code å­—æ®µï¼Œä½¿ç”¨ç¬¬ä¸€ç¬”ä»·æ ¼
                    df['pre_close'] = df['price'].iloc[0]
        
        # é¢„å…ˆç¼“å­˜éœ€è¦çš„é«˜é€Ÿåˆ— (NumPy arrays) - æ€§èƒ½å…³é”®ä¼˜åŒ–
        df['_datetime_values'] = df['datetime'].values
        df['_price_values'] = df['price'].values
        df['_vol_values'] = df['vol'].values if 'vol' in df.columns else np.zeros(len(df))
        df['_cum_vol_values'] = df['cum_volume'].values if 'cum_volume' in df.columns else np.zeros(len(df))
        
        return df
    
    def lazy_load_stock(self, stock_code: str) -> pd.DataFrame:
        """
        æŒ‰éœ€åŠ è½½å•åªè‚¡ç¥¨æ•°æ®(èŠ‚çœå†…å­˜)
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            è‚¡ç¥¨æ•°æ®DataFrame
        """
        if stock_code not in self.all_data:
            file_path = self.data_dir / f"{stock_code}.parquet"
            if file_path.exists():
                try:
                    df = pd.read_parquet(file_path)
                    df = self._preprocess_tick_data(df)
                    self.all_data[stock_code] = df
                except Exception as e:
                    logging.warning(f"åŠ è½½ {stock_code} å¤±è´¥: {e}")
        
        return self.all_data.get(stock_code)
    
    def _load_single_stock(self, file_path: Path) -> tuple:
        """
        åŠ è½½å•åªè‚¡ç¥¨æ•°æ®ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰
        
        Args:
            file_path: parquetæ–‡ä»¶è·¯å¾„
            
        Returns:
            (stock_code, dataframe) æˆ– (stock_code, None) å¦‚æœå¤±è´¥
        """
        stock_code = file_path.stem
        try:
            df = pd.read_parquet(file_path)
            df = self._preprocess_tick_data(df)
            return (stock_code, df)
        except Exception as e:
            logging.warning(f"åŠ è½½ {stock_code} å¤±è´¥: {e}")
            return (stock_code, None)
    
    def load_all_stocks_parallel(self, max_workers: int = 8, progress_callback=None) -> int:
        """
        å¤šçº¿ç¨‹å¹¶è¡ŒåŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        
        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total)
            
        Returns:
            æˆåŠŸåŠ è½½çš„è‚¡ç¥¨æ•°é‡
        """
        # ========================================
        # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šä¼˜å…ˆä½¿ç”¨å•æ–‡ä»¶å¿«é€ŸåŠ è½½
        # ========================================
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨åˆå¹¶çš„å•æ–‡ä»¶æ ¼å¼ 
        # æƒ…å†µ1: self.data_dir æ˜¯ tick/ æ–‡ä»¶å¤¹ï¼Œåˆå¹¶æ–‡ä»¶åœ¨çˆ¶ç›®å½•
        # æƒ…å†µ2: self.data_dir å°±æ˜¯æ—¥æœŸç›®å½•ï¼Œåˆå¹¶æ–‡ä»¶å°±åœ¨æ­¤å¤„
        tick_data_file = self.data_dir / "tick_data.parquet" if not self.data_dir.name == 'tick' else self.data_dir.parent / "tick_data.parquet"
        
        if tick_data_file.exists():
            logging.info(f"âš¡ æ£€æµ‹åˆ°ä¼˜åŒ–æ ¼å¼ï¼Œä½¿ç”¨å¿«é€ŸåŠ è½½: {tick_data_file}")
            return self._load_from_single_file(tick_data_file, progress_callback)
        
        # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„å¤šæ–‡ä»¶åŠ è½½ (å•ä¸ªè‚¡ç¥¨ä¸€ä¸ªæ–‡ä»¶)
        parquet_files = list(self.data_dir.glob("*.parquet"))
        
        # æ’é™¤æ‰åˆå¹¶æ–‡ä»¶ï¼Œä»¥é˜²ä¸‡ä¸€éå†åˆ°äº† (è™½ç„¶æ¦‚ç‡æä½)
        parquet_files = [f for f in parquet_files if f.name != "tick_data.parquet"]
        
        total = len(parquet_files)
        loaded_count = 0
        
        if total == 0:
            logging.warning(f"ç›®å½• {self.data_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„parquetæ–‡ä»¶")
            return 0
        
        logging.info(f"å¼€å§‹å¤šçº¿ç¨‹åŠ è½½ {total} åªè‚¡ç¥¨æ•°æ®ï¼Œçº¿ç¨‹æ•°: {max_workers}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {executor.submit(self._load_single_stock, f): f for f in parquet_files}
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for idx, future in enumerate(as_completed(future_to_file), 1):
                stock_code, df = future.result()
                
                if df is not None:
                    self.all_data[stock_code] = df
                    loaded_count += 1
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(idx, total)
        
        logging.info(f"æ•°æ®åŠ è½½å®Œæˆ: {loaded_count}/{total}")
        return loaded_count
    
    def _load_from_single_file(self, tick_data_file: Path, progress_callback=None) -> int:
        """
        ä»åˆå¹¶çš„å•ä¸ªparquetæ–‡ä»¶å¿«é€ŸåŠ è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        (æè‡´ä¼˜åŒ–ç‰ˆï¼šå‘é‡åŒ–é¢„å¤„ç† + æé€Ÿæ‹†åˆ†)
        """
        import time
        start_time = time.time()
        
        logging.info(f"âš¡ å¿«é€ŸåŠ è½½æ¨¡å¼ï¼šæ­£åœ¨è¯»å–æ•°æ®...")
        
        # 1. æé€Ÿè¯»å–
        df = pd.read_parquet(tick_data_file)
        
        read_time = time.time() - start_time
        logging.info(f"   è¯»å–å®Œæˆ: {read_time:.2f}ç§’ (è¡Œæ•°: {len(df):,})")
        
        process_start = time.time()
        logging.info(f"   æ­£åœ¨è¿›è¡Œå…¨é‡å‘é‡åŒ–é¢„å¤„ç†...")

        # 2. å…¨é‡é¢„å¤„ç† (Vectorized Preprocessing) - åœ¨å¾ªç¯å¤–ä¸€æ¬¡æ€§å®Œæˆ
        
        # A. ç¡®ä¿æ—¶é—´åˆ— 
        if 'datetime' not in df.columns:
            if 'date' in df.columns and 'time' in df.columns:
                df['datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'].astype(str))
            elif 'time' in df.columns:
                from datetime import date as dt_date
                today = dt_date.today().strftime('%Y%m%d')
                df['datetime'] = pd.to_datetime(today + ' ' + df['time'].astype(str))
        
        # B. å‘é‡åŒ–è®¡ç®—ç´¯è®¡æˆäº¤é‡
        if 'vol' in df.columns:
            # GroupBy + CumSum é€Ÿåº¦éå¸¸å¿«
            df['cum_volume'] = df.groupby('stock_code')['vol'].cumsum()
        
        # C. å¤„ç†æ˜¨æ”¶ä»·
        if 'pre_close' in df.columns:
            # å¦‚æœä¸‹è½½çš„æ•°æ®ä¸­å·²ç»åŒ…å«äº†è¡¥ä¸åçš„æ˜¨æ”¶ä»·ï¼Œç›´æ¥ä½¿ç”¨
            # ç¡®ä¿ç±»å‹æ­£ç¡®
            df['pre_close'] = df['pre_close'].astype('float32')
        elif self.pre_close_map and 'stock_code' in df.columns:
            # ç¡®ä¿ä»£ç æ ¼å¼ä¸€è‡´
            df['stock_code'] = df['stock_code'].astype(str).str.zfill(6)
            # æ˜ å°„æ˜¨æ”¶ä»·
            df['pre_close'] = df['stock_code'].map(self.pre_close_map).astype('float32')
            
            # å¯¹æœªåŒ¹é…åˆ°çš„å¡«å……æ¯ç»„çš„ç¬¬ä¸€ç¬”ä»·æ ¼
            if df['pre_close'].isnull().any():
                if 'price' in df.columns:
                    first_prices = df.groupby('stock_code')['price'].transform('first')
                    df['pre_close'] = df['pre_close'].fillna(first_prices)
        elif 'price' in df.columns:
             # å¦‚æœæ²¡æœ‰æ˜¨æ”¶ä»·è¡¨ï¼Œä¸”æ•°æ®ä¸­ä¹Ÿæ²¡æœ‰æ˜¨æ”¶ä»·åˆ—ï¼Œå…¨éƒ¨ä½¿ç”¨ç¬¬ä¸€ç¬”ä»·æ ¼
             df['pre_close'] = df.groupby('stock_code')['price'].transform('first')
        
        process_time = time.time() - process_start
        logging.info(f"   é¢„å¤„ç†å®Œæˆ: {process_time:.2f}ç§’ (å‘é‡åŒ–)")
        
        # D. åˆ†ç§’å¹³æ»‘å¤„ç† (Plan A)
        # å¦‚æœæ•°æ®åªæœ‰åˆ†é’Ÿç²¾åº¦ï¼Œå°†åŒä¸€åˆ†é’Ÿå†…çš„å¤šç¬”æˆäº¤å‡åŒ€åˆ†å¸ƒåœ¨ 60 ç§’å†…
        logging.info(f"   æ­£åœ¨æ‰§è¡Œåˆ†ç§’å¹³æ»‘å¤„ç† (Plan A)...")
        # ç¡®ä¿æ•°æ®æœ‰åº
        df = df.sort_values(['stock_code', 'datetime']).reset_index(drop=True)
        
        df['_cum_count'] = df.groupby(['stock_code', 'datetime']).cumcount()
        df['_total_in_min'] = df.groupby(['stock_code', 'datetime'])['price'].transform('count')
        
        # åªæœ‰åœ¨æ£€æµ‹åˆ°æ˜¯åˆ†é’Ÿçº§æ•°æ®ï¼ˆç§’æ•°ä¸º0ï¼‰æ—¶æ‰å¹³æ»‘
        if not df.empty and df['datetime'].iloc[0].second == 0:
            # æ€§èƒ½å…³é”®ï¼šä½¿ç”¨å‘é‡åŒ–åŠ æ³•
            df['datetime'] = df['datetime'] + pd.to_timedelta(
                (df['_cum_count'] * 60 / df['_total_in_min']).astype(int), unit='s'
            )
            
        df.drop(columns=['_cum_count', '_total_in_min'], inplace=True)
        
        # 3. æé€Ÿæ‹†åˆ†
        split_start = time.time()
        total_stocks = df['stock_code'].nunique()
        logging.info(f"   æ­£åœ¨æ‹†åˆ†ä¸º {total_stocks} åªè‚¡ç¥¨...")
        
        loaded_count = 0
        has_vol = 'vol' in df.columns
        has_cum_vol = 'cum_volume' in df.columns
        
        # ä½¿ç”¨ groupby è¿­ä»£æ‹†åˆ†
        for stock_code, group_df in df.groupby('stock_code'):
            # å…³é”®ä¿®å¤: å¿…é¡» reset_index
            stock_df = group_df.reset_index(drop=True)
            
            self.all_data[stock_code] = stock_df
            
            # --- æ„å»ºæé€Ÿç¼“å­˜ (Pure NumPy) ---
            # æå– float32 æ•°ç»„ä»¥èŠ‚çœå†…å­˜å¹¶åŠ é€Ÿ
            t_values = stock_df['datetime'].values
            p_values = stock_df['price'].values
            v_values = stock_df['cum_volume'].values if has_cum_vol else None
            
            # æå–æ˜¨æ”¶ä»· (æ ‡é‡)
            pre_close = float(stock_df['pre_close'].iloc[0]) if 'pre_close' in stock_df.columns else float(p_values[0])
            
            self.fast_data_cache[stock_code] = (t_values, p_values, v_values, pre_close)
            
            loaded_count += 1
            
            if progress_callback and loaded_count % 1000 == 0:
                 progress_callback(loaded_count, total_stocks)
                 
        if progress_callback:
            progress_callback(total_stocks, total_stocks)
            
        split_time = time.time() - split_start
        total_time = time.time() - start_time
        
        logging.info(f"   æ‹†åˆ†ä¸ç¼“å­˜: {split_time:.2f}ç§’")
        logging.info(f"âœ… æé€ŸåŠ è½½å®Œæˆ: {total_time:.2f}ç§’!")
        
        return loaded_count
    
    def get_snapshot_at_time(self, target_time: datetime) -> Dict:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹çš„å¸‚åœºå¿«ç…§ - æé€Ÿç‰ˆ (Pure NumPy)
        
        å®Œå…¨ç»•è¿‡ Pandas DataFrameï¼Œç›´æ¥æ“ä½œé¢„ç¼“å­˜çš„ NumPy æ•°ç»„ã€‚
        æ€§èƒ½æå‡ç›®æ ‡ï¼šæ¯”åŸæœ‰é€»è¾‘å¿« 10-50 å€ã€‚
        """
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆç²¾ç¡®åˆ°ç§’ï¼‰
        time_key = target_time.strftime('%Y%m%d_%H%M%S')
        
        # æ£€æŸ¥å¿«ç…§ç¼“å­˜ (LRU)
        if time_key in self.snapshot_cache:
            # æ›´æ–°LRUé¡ºåº
            if time_key in self.snapshot_cache_order:
                self.snapshot_cache_order.remove(time_key)
            self.snapshot_cache_order.append(time_key)
            
            self.current_time = target_time
            return self.snapshot_cache[time_key]
        
        self.current_time = target_time
        
        # åˆå§‹åŒ–ç´¢å¼•ç¼“å­˜
        if not hasattr(self, 'index_cache'):
            self.index_cache = {code: 0 for code in self.fast_data_cache.keys()}
        
        # æ£€æµ‹æ—¶é—´å›é€€ï¼Œé‡ç½®ç´¢å¼•ç¼“å­˜
        if hasattr(self, 'last_snapshot_time') and target_time < self.last_snapshot_time:
            self.index_cache = {code: 0 for code in self.fast_data_cache.keys()}
        
        self.last_snapshot_time = target_time
        
        snapshot = {
            'time': target_time,
            'stocks': {},
            'stats': {
                'total_stocks': 0,
                'up_count': 0,
                'down_count': 0,
                'flat_count': 0,
                'limit_up_count': 0,
                'limit_down_count': 0,
            }
        }
        
        # è½¬æ¢ä¸º numpy.datetime64[ns] ä»¥åŒ¹é… Pandas çš„é»˜è®¤ç²¾åº¦
        target_np = np.array(target_time, dtype='datetime64[ns]')
        
        # éå†æé€Ÿç¼“å­˜ (Pure NumPy Loop)
        # è¿™é‡Œçš„ items() è¿­ä»£é€Ÿåº¦è¿œå¿«äº DataFrame çš„ items æˆ– iterrows
        for stock_code, (times, price_vals, vol_vals, pre_close) in self.fast_data_cache.items():
            if len(times) == 0:
                continue

            # è·å–ä¸Šæ¬¡æŸ¥æ‰¾çš„ç´¢å¼•ä½ç½®
            last_idx = self.index_cache.get(stock_code, 0)
            
            # --- æé€Ÿç´¢å¼•æŸ¥æ‰¾ ---
            # ä¼˜åŒ–é€»è¾‘ï¼šæ™ºèƒ½åˆ‡æ¢çº¿æ€§æ‰«æå’ŒäºŒåˆ†æŸ¥æ‰¾
            # 1. æ­£å¸¸å›æ”¾ï¼ˆæ—¶é—´å¾®å¢ï¼‰ï¼šåªèƒ½çº¿æ€§æ‰«æï¼ˆéå¸¸å¿«ï¼‰
            # 2. æ‹–åŠ¨æ»‘å—ï¼ˆå¤§å¹…è·³è½¬ï¼‰ï¼šå¼ºåˆ¶äºŒåˆ†æŸ¥æ‰¾ï¼ˆé¿å…æ•°åƒæ¬¡å¾ªç¯ï¼‰
            
            should_scan_linearly = False
            
            if last_idx < len(times) and target_np >= times[last_idx]:
                # åªæœ‰å½“ç›®æ ‡æ—¶é—´åœ¨å½“å‰ä½ç½®çš„"é™„è¿‘"æ—¶ï¼Œæ‰ä½¿ç”¨çº¿æ€§æ‰«æ
                # è®¾å®šé˜ˆå€¼ï¼šä¾‹å¦‚æ£€æŸ¥å¾€å20ä¸ªç‚¹çš„ä½ç½®
                lookahead = 20
                if last_idx + lookahead >= len(times):
                    # å‰©ä½™æ•°æ®ä¸è¶³20ä¸ªï¼Œç›´æ¥çº¿æ€§æ‰«å®Œ
                    should_scan_linearly = True
                elif times[last_idx + lookahead] >= target_np:
                    # å¦‚æœå¾€å20ä¸ªç‚¹çš„æ—¶é—´å·²ç»è¶…è¿‡ç›®æ ‡æ—¶é—´ï¼Œè¯´æ˜ç›®æ ‡å°±åœ¨è¿™20ä¸ªç‚¹ä¹‹å†…
                    # æ­¤æ—¶çº¿æ€§æ‰«ææ¯”äºŒåˆ†æŸ¥æ‰¾æ›´å¿«
                    should_scan_linearly = True
                # else: ç›®æ ‡åœ¨20ä¸ªç‚¹ä¹‹å¤–ï¼Œæ„å‘³ç€å‘ç”Ÿäº†è¾ƒå¤§è·¨åº¦è·³è½¬ -> ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾
            
            if should_scan_linearly:
                # å‘å‰çº¿æ€§æ‰«æ
                idx = last_idx
                # ä½¿ç”¨ numpy çš„é€å…ƒç´ æ¯”è¾ƒé€šå¸¸æ¯” python å¾ªç¯å¿«ï¼Œä½†åœ¨å°èŒƒå›´å†… Python å¾ªç¯ overhead ä¹Ÿä¸å¤§
                # ä¸ºäº†æè‡´æ€§èƒ½ï¼Œä¿æŒåŸé€»è¾‘ä½†æœ‰èŒƒå›´é™åˆ¶
                while idx + 1 < len(times) and times[idx + 1] <= target_np:
                    idx += 1
            else:
                # æ—¶é—´å›é€€æˆ–å¤§å¹…åº¦è·³è·ƒï¼Œä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾
                idx = np.searchsorted(times, target_np, side='right') - 1
            
            # æ›´æ–°ç´¢å¼•ç¼“å­˜
            self.index_cache[stock_code] = max(0, idx)
            
            if idx >= 0:
                # ç›´æ¥è®¿é—® NumPy æ•°ç»„ (æå¿«)
                current_price = price_vals[idx]
                
                # å¦‚æœæ²¡æœ‰volæ•°æ®ï¼Œè®¾ä¸º0
                cum_volume = vol_vals[idx] if vol_vals is not None else 0
                
                # è®¡ç®—æ¶¨è·Œå¹…
                if pre_close > 0:
                    pct_change = (current_price - pre_close) / pre_close * 100
                    # è®¡ç®—å¼€ç›˜æ¶¨è·Œå¹… (ä½¿ç”¨å½“æ—¥å‰å‡ ç¬”ä½œä¸ºå¼€ç›˜ä»·)
                    open_price = price_vals[0]
                    open_pct_change = (open_price - pre_close) / pre_close * 100
                else:
                    pct_change = 0.0
                    open_pct_change = 0.0
                
                snapshot['stocks'][stock_code] = {
                    'price': float(current_price),
                    'open_price': float(open_price),
                    'volume': float(cum_volume),
                    'pct_change': float(pct_change),
                    'open_pct_change': float(open_pct_change),
                }
                
                # ç»Ÿè®¡æ¶¨è·Œ
                if pct_change > 0.001:
                    snapshot['stats']['up_count'] += 1
                elif pct_change < -0.001:
                    snapshot['stats']['down_count'] += 1
                else:
                    snapshot['stats']['flat_count'] += 1
                
                # ç»Ÿè®¡æ¶¨è·Œåœ
                # 1. åˆ¤å®šæ¶¨è·Œå¹…æ¯”ä¾‹
                if stock_code.startswith(('688', '300', '689')):
                    ratio = 0.2
                elif stock_code.startswith(('8', '4', '92')):
                    ratio = 0.3
                else:
                    ratio = 0.1
                    # ä¸»æ¿ ST è‚¡ 5%
                    if "ST" in self.stock_name_map.get(stock_code, ""):
                        ratio = 0.05
                
                # 2. è®¡ç®—æ¶¨è·Œåœä»·æ ¼ (åŒ detect_limit_movements é€»è¾‘)
                limit_up = round(pre_close * (1 + ratio) + 0.0001, 2)
                limit_down = round(pre_close * (1 - ratio) + 0.0001, 2)
                
                if current_price >= limit_up:
                    snapshot['stats']['limit_up_count'] += 1
                elif current_price <= limit_down:
                    snapshot['stats']['limit_down_count'] += 1
        
        snapshot['stats']['total_stocks'] = len(snapshot['stocks'])
        
        # å­˜å…¥ç¼“å­˜
        self.snapshot_cache[time_key] = snapshot
        self.snapshot_cache_order.append(time_key)
        
        # ç»´æŠ¤ç¼“å­˜å¤§å°ï¼ˆLRUæ·˜æ±°ï¼‰
        if len(self.snapshot_cache_order) > self.snapshot_cache_size:
            oldest_key = self.snapshot_cache_order.pop(0)
            if oldest_key in self.snapshot_cache:
                del self.snapshot_cache[oldest_key]
                logging.debug(f"âœ‚ï¸ ç§»é™¤è¿‡æœŸå¿«ç…§: {oldest_key}")
        
        return snapshot
    
    def calculate_stock_rankings(self, snapshot: Dict, top_n: int = 50) -> pd.DataFrame:
        """
        è®¡ç®—ä¸ªè‚¡æ¶¨å¹…æ’è¡Œ - ä¼˜åŒ–ç‰ˆ (åˆ©ç”¨é¢„å¤„ç†å¥½çš„æ•°ç»„)
        """
        if not snapshot.get('stocks'):
            return pd.DataFrame(columns=['stock_code', 'stock_name', 'price', 'pct_change', 'volume'])
        
        # æå–æ•°æ®
        stocks_data = snapshot['stocks']
        codes = list(stocks_data.keys())
        
        # å‘é‡åŒ–æ„å»º DataFrame (æ¯”å¾ªç¯å¿«)
        df = pd.DataFrame({
            'stock_code': codes,
            'price': [d['price'] for d in stocks_data.values()],
            'pct_change': [d['pct_change'] for d in stocks_data.values()],
            'volume': [d['volume'] for d in stocks_data.values()]
        })
        
        # æ˜ å°„åç§°
        df['stock_name'] = df['stock_code'].map(self.stock_name_map)
        
        # æ’åºå¹¶æˆªæ–­
        df = df.sort_values('pct_change', ascending=False).head(top_n)
        df = df.reset_index(drop=True)
        df.index += 1
        
        return df
    
    def calculate_sector_rankings(self, snapshot: Dict, sector_type: str = 'industry', top_n: int = 20) -> pd.DataFrame:
        """
        è®¡ç®—æ¿å—æ¶¨å¹…æ’è¡Œ - ä¼˜åŒ–ç‰ˆ
        """
        # é€‰æ‹©æ˜ å°„è¡¨
        if sector_type == 'industry':
            sector_map = self.industry_map
        elif sector_type == 'concept':
            sector_map = self.concept_map
        elif sector_type == 'region':
            sector_map = self.region_map
        else:
            sector_map = self.industry_map
        
        if not snapshot.get('stocks'):
            return pd.DataFrame(columns=['sector', 'avg_pct_change', 'stock_count', 'total_volume', 'sector_type'])

        sector_stats = defaultdict(lambda: {'total_pct': 0.0, 'count': 0, 'volume': 0.0})
        
        # èšåˆæ¿å—æ•°æ® (ä¼˜åŒ–å¾ªç¯)
        for code, data in snapshot['stocks'].items():
            sectors = sector_map.get(code)
            if not sectors:
                continue
            
            # ä¿®æ­£å¼‚å¸¸æ¶¨å¹…è´¡çŒ®
            stock_pct = data.get('pct_change', 0)
            if abs(stock_pct) > 30:
                open_price = data.get('open_price', 0)
                current_price = data.get('price', 0)
                stock_pct = ((current_price - open_price) / open_price * 100) if open_price > 0 else 0
            
            vol = data.get('volume', 0)
            
            for sector in sectors:
                stat = sector_stats[sector]
                stat['total_pct'] += stock_pct
                stat['count'] += 1
                stat['volume'] += vol
        
        # è½¬åŒ–ä¸º DataFrame
        if not sector_stats:
            return pd.DataFrame(columns=['sector', 'avg_pct_change', 'stock_count', 'total_volume', 'sector_type'])
            
        res = []
        for sector, stat in sector_stats.items():
            if stat['count'] > 0:
                res.append({
                    'sector': sector,
                    'avg_pct_change': stat['total_pct'] / stat['count'],
                    'stock_count': stat['count'],
                    'total_volume': stat['volume'],
                    'sector_type': sector_type
                })
        
        df = pd.DataFrame(res)
        df = df.sort_values('avg_pct_change', ascending=False).head(top_n)
        df = df.reset_index(drop=True)
        df.index += 1
        
        return df
    
    def detect_rapid_rise(self, time_window_minutes: int = 5, 
                          pct_threshold: float = 3.0) -> List[Dict]:
        """
        æ£€æµ‹å¿«é€Ÿæ‹‰å‡ä¸ªè‚¡ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰
        
        Args:
            time_window_minutes: æ—¶é—´çª—å£(åˆ†é’Ÿ)
            pct_threshold: æ¶¨å¹…é˜ˆå€¼(%)
            
        Returns:
            æ‹‰å‡è‚¡ç¥¨åˆ—è¡¨
        """
        return self.detect_abnormal_movement(
            time_window_minutes=time_window_minutes,
            rise_threshold=pct_threshold,
            fall_threshold=None,  # åªæ£€æµ‹æ¶¨å¹…
            volume_threshold=None
        )
    
    def detect_abnormal_movement(self, time_window_minutes: int = 5, 
                                 rise_threshold: float = 3.0,
                                 fall_threshold: float = -3.0,
                                 volume_threshold: float = None) -> List[Dict]:
        """
        æ£€æµ‹å¼‚åŠ¨ä¸ªè‚¡ - è¶…é«˜é€Ÿç‰ˆ (O(N_stocks * log N) + ç¼“å­˜ä¼˜åŒ–)
        
        ä½¿ç”¨NumPyæ•°ç»„ç¼“å­˜å’ŒäºŒåˆ†æŸ¥æ‰¾ï¼Œæ€§èƒ½æå‡10å€ä»¥ä¸Š
        
        Args:
            time_window_minutes: æ—¶é—´çª—å£(åˆ†é’Ÿ)
            rise_threshold: æ¶¨å¹…é˜ˆå€¼(%)ï¼ŒNoneè¡¨ç¤ºä¸ç›‘æ§æ¶¨å¹…
            fall_threshold: è·Œå¹…é˜ˆå€¼(%)ï¼Œåº”ä¸ºè´Ÿæ•°ï¼ŒNoneè¡¨ç¤ºä¸ç›‘æ§è·Œå¹…
            volume_threshold: æˆäº¤é¢é˜ˆå€¼(ä¸‡å…ƒ)ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            
        Returns:
            å¼‚åŠ¨è‚¡ç¥¨åˆ—è¡¨
        """
        abnormal_stocks = []
        
        if self.current_time is None:
            return abnormal_stocks
        
        time_window_start = self.current_time - pd.Timedelta(minutes=time_window_minutes)
        # å…³é”®ä¿®æ­£ï¼šç¡®ä¿ä¸ times æ•°ç»„çš„ datetime64[ns] ç²¾åº¦ä¸€è‡´
        start_np = np.array(time_window_start, dtype='datetime64[ns]')
        end_np = np.array(self.current_time, dtype='datetime64[ns]')
        
        # éå†æé€Ÿç¼“å­˜ (Pure NumPy Loop)
        # items() è¿­ä»£æ¯” DataFrame items æå¿«
        for stock_code, (times, price_vals, cum_vol_vals, _) in self.fast_data_cache.items():
            if len(times) == 0:
                continue
            
            # ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾å®šä½çª—å£è¾¹ç•Œ
            start_idx = np.searchsorted(times, start_np, side='left')
            end_idx = np.searchsorted(times, end_np, side='right') - 1
            
            if end_idx > start_idx and start_idx >= 0 and end_idx < len(times):
                start_price = price_vals[start_idx]
                end_price = price_vals[end_idx]
                
                if start_price > 0:
                    pct_change = (end_price - start_price) / start_price * 100
                    
                    # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶
                    is_abnormal = False
                    movement_type = None
                    
                    # æ£€æŸ¥æ¶¨å¹…
                    if rise_threshold is not None and pct_change >= rise_threshold:
                        is_abnormal = True
                        movement_type = 'rise'
                    
                    # æ£€æŸ¥è·Œå¹…
                    if fall_threshold is not None and pct_change <= fall_threshold:
                        is_abnormal = True
                        movement_type = 'fall'
                    
                    if is_abnormal:
                        # è®¡ç®—æˆäº¤é¢ï¼ˆä¸‡å…ƒï¼‰- ä½¿ç”¨ç´¯è®¡æˆäº¤é‡å·®å€¼ (O(1)å¤æ‚åº¦)
                        if cum_vol_vals is not None:
                            # ç´¯è®¡é‡å·®å€¼ = ç»“æŸæ—¶åˆ»ç´¯è®¡ - å¼€å§‹å‰æ—¶åˆ»ç´¯è®¡
                            vol_end = cum_vol_vals[end_idx]
                            vol_start = cum_vol_vals[start_idx - 1] if start_idx > 0 else 0
                            window_vol = vol_end - vol_start
                        else:
                            window_vol = 0
                            
                        # æˆäº¤é‡å•ä½æ˜¯æ‰‹(100è‚¡)ï¼Œä»·æ ¼å•ä½æ˜¯å…ƒ
                        volume_amount = window_vol * 100 * end_price / 10000
                        
                        # æˆäº¤é¢è¿‡æ»¤
                        if volume_threshold is not None and volume_amount < volume_threshold:
                            continue
                        
                        abnormal_stocks.append({
                            'stock_code': stock_code,
                            'movement_type': movement_type,
                            'start_price': start_price,
                            'end_price': end_price,
                            'pct_change': pct_change,
                            'volume_amount': volume_amount,
                            'start_time': pd.Timestamp(times[start_idx]),
                            'end_time': pd.Timestamp(times[end_idx]),
                        })
        
        # æŒ‰æ¶¨è·Œå¹…ç»å¯¹å€¼æ’åºï¼ˆå¼‚åŠ¨å¹…åº¦å¤§çš„åœ¨å‰ï¼‰
        abnormal_stocks.sort(key=lambda x: abs(x['pct_change']), reverse=True)
        
        return abnormal_stocks

    def detect_limit_movements(self) -> List[Dict]:
        """
        æ£€æµ‹æ¶¨è·Œåœå¼‚åŠ¨ï¼ˆå°æ¿/ç‚¸æ¿ï¼‰
        
        Returns:
            æ¶¨è·Œåœå¼‚åŠ¨åˆ—è¡¨
        """
        limit_events = []
        if self.current_time is None:
            return limit_events

        end_np = np.array(self.current_time, dtype='datetime64[ns]')

        for stock_code, (times, price_vals, _, pre_close) in self.fast_data_cache.items():
            if len(times) == 0:
                continue

            # å¯»æ‰¾å½“å‰æ—¶é—´ç‚¹å¯¹åº”çš„æœ€æ–° Tick
            end_idx = np.searchsorted(times, end_np, side='right') - 1
            if end_idx < 0:
                continue

            # åªæœ‰åœ¨ Tick åˆšåˆšå‘ç”Ÿå˜åŒ–æ—¶æ‰è§¦å‘å¼‚åŠ¨ï¼ˆé¿å…é‡å¤è§¦å‘ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œçš„é€»è¾‘å‡è®¾è°ƒç”¨æ–¹ä¼šæ ¹æ®æ—¶é—´æ¨ç§»æŒç»­è°ƒç”¨
            # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬æ£€æµ‹ end_idx å¯¹åº”çš„ Tick æ—¶é—´æ˜¯å¦å°±æ˜¯å½“å‰â€œæ¨¡æ‹Ÿç§’â€æˆ–è€…æ˜¯æœ€è¿‘å‡ ç§’å†…
            tick_time = pd.Timestamp(times[end_idx])
            if (self.current_time - tick_time).total_seconds() >= 3:
                # å¦‚æœè¿™ä¸ª Tick å·²ç»æ˜¯ 3 ç§’å‰çš„äº†ï¼Œè¯´æ˜æ˜¯è€æ•°æ®ï¼Œä¸è§†ä½œâ€œæ–°å¼‚åŠ¨â€
                #ï¼ˆé™¤éæ˜¯åˆšå¼€ç›˜æˆ–è€…æ•°æ®æ–­æµï¼Œè¿™é‡Œæƒè¡¡ä¸€ä¸‹ï¼‰
                continue

            current_price = price_vals[end_idx]
            
            # è®¡ç®—è¯¥è‚¡çš„æ¶¨è·Œåœä»·æ ¼
            if stock_code.startswith(('688', '300', '689')):
                ratio = 0.2
            elif stock_code.startswith(('8', '4', '92')):
                ratio = 0.3
            else:
                ratio = 0.1
                # åªæœ‰ä¸»æ¿çš„ ST è‚¡æ‰æ˜¯ 5% é™åˆ¶ï¼Œåˆ›ä¸šæ¿å’Œç§‘åˆ›æ¿ ST ä»æ˜¯ 20%
                stock_name = self.stock_name_map.get(stock_code, "")
                if "ST" in stock_name:
                    ratio = 0.05
                
            # Aè‚¡æ¶¨è·Œåœè®¡ç®—é€šå¸¸æ˜¯å››èˆäº”å…¥åˆ°åˆ†ï¼Œä½†ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ 0.005 çš„åç§»
            limit_up = round(pre_close * (1 + ratio) + 0.0001, 2)
            limit_down = round(pre_close * (1 - ratio) + 0.0001, 2)
            
            event_type = None
            desc = ""
            
            if end_idx > 0:
                prev_price = price_vals[end_idx - 1]
                was_at_limit_up = prev_price >= limit_up
                was_at_limit_down = prev_price <= limit_down
                
                is_at_limit_up = current_price >= limit_up
                is_at_limit_down = current_price <= limit_down
                
                if not was_at_limit_up and is_at_limit_up:
                    event_type = "hit_limit_up"
                    desc = "ğŸš€ å°æ¶¨åœ"
                elif was_at_limit_up and not is_at_limit_up:
                    event_type = "break_limit_up"
                    desc = "ğŸ’¥ ç‚¸æ¶¨åœ"
                elif not was_at_limit_down and is_at_limit_down:
                    event_type = "hit_limit_down"
                    desc = "ğŸ“‰ å°è·Œåœ"
                elif was_at_limit_down and not is_at_limit_down:
                    event_type = "break_limit_down"
                    desc = "â™»ï¸ ç‚¸è·Œåœ"
            else:
                # å¼€ç›˜ç¬¬ä¸€ç¬”
                if current_price >= limit_up:
                    event_type = "hit_limit_up"
                    desc = "ğŸš€ æ¶¨åœå¼€ç›˜"
                elif current_price <= limit_down:
                    event_type = "hit_limit_down"
                    desc = "ğŸ“‰ è·Œåœå¼€ç›˜"
            
            if event_type:
                limit_events.append({
                    'stock_code': stock_code,
                    'stock_name': stock_name,
                    'event_type': event_type,
                    'desc': desc,
                    'price': current_price,
                    'time': tick_time.strftime('%H:%M:%S'),
                    'pct_change': (current_price - pre_close) / pre_close * 100
                })
        
        return limit_events
    
    def load_sector_mappings(self):
        """åŠ è½½è¡Œä¸šã€æ¦‚å¿µã€åœ°åŒºæ˜ å°„"""
        import os
        
        source = SECTOR_MAPPING_CONFIG.get('source', 'iwencai')
        logging.info(f"æ­£åœ¨ä» {source} åŠ è½½æ¿å—æ˜ å°„...")
        
        if source == 'iwencai':
            files = SECTOR_MAPPING_CONFIG['iwencai_files']
            self._load_iwencai_mapping(files['industry'], self.industry_map, "è¡Œä¸š")
            self._load_iwencai_mapping(files['concept'], self.concept_map, "æ¦‚å¿µ")
            self._load_iwencai_mapping(files['region'], self.region_map, "åœ°åŒº")
        else:
            files = SECTOR_MAPPING_CONFIG['eastmoney_files']
            self._load_eastmoney_mapping(files['industry'], self.industry_map, "è¡Œä¸š")
            self._load_eastmoney_mapping(files['concept'], self.concept_map, "æ¦‚å¿µ")
            self._load_eastmoney_mapping(files['region'], self.region_map, "åœ°åŒº")

    def _load_iwencai_mapping(self, file_path: str, target_map: Dict, label: str):
        """åŠ è½½ iwencai æ ¼å¼çš„æ˜ å°„æ–‡ä»¶"""
        import os
        if not os.path.exists(file_path):
            logging.warning(f"æœªæ‰¾åˆ° iWencai {label}æ–‡ä»¶: {file_path}")
            return
            
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            for encoding in ['utf-8-sig', 'gbk', 'utf-8']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    # æ¸…ç†åˆ—å
                    df.columns = [c.strip() for c in df.columns]
                    break
                except Exception:
                    continue
            else:
                logging.error(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥ç¼–ç ")
                return

            # å¯»æ‰¾ä»£ç åˆ—
            code_col = None
            for col in ['stock_code', 'ä»£ç ', 'è¯åˆ¸ä»£ç ']:
                if col in df.columns:
                    code_col = col
                    break
            
            # å¯»æ‰¾åç§°åˆ—
            name_col = None
            for col in ['classification_name', 'åç§°', 'æ¿å—åç§°', 'concept_name']:
                if col in df.columns:
                    name_col = col
                    break
            
            if not code_col or not name_col:
                logging.warning(f"iWencai {label}æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡® (ç¼ºå¤±ä»£ç æˆ–åç§°åˆ—): {file_path}")
                return

            count = 0
            for _, row in df.iterrows():
                val = row[code_col]
                if pd.isna(val): continue
                
                code = str(val).split('.')[0].zfill(6)
                name = str(row[name_col]).strip()
                
                if name == 'nan' or not name: continue
                
                if code not in target_map:
                    target_map[code] = []
                if name not in target_map[code]:
                    target_map[code].append(name)
                    count += 1
            
            logging.info(f"âœ… iWencai {label}æ˜ å°„åŠ è½½å®Œæˆ, å…±åŠ è½½ {count} æ¡æ˜ å°„, è¦†ç›– {len(target_map)} åªè‚¡ç¥¨")
        except Exception as e:
            logging.warning(f"åŠ è½½ iWencai {label}æ˜ å°„å¤±è´¥: {e}")

    def _load_eastmoney_mapping(self, file_path: str, target_map: Dict, label: str):
        """åŠ è½½ eastmoney æ ¼å¼çš„æ˜ å°„æ–‡ä»¶"""
        import os
        if not os.path.exists(file_path):
            logging.warning(f"æœªæ‰¾åˆ° Eastmoney {label}æ–‡ä»¶: {file_path}")
            return
            
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            for encoding in ['utf-8-sig', 'gbk', 'utf-8']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    df.columns = [c.strip() for c in df.columns]
                    break
                except Exception:
                    continue
            else:
                return

            # å¯»æ‰¾ä»£ç åˆ—
            code_col = None
            for col in ['stock_code', 'ä»£ç ', 'è¯åˆ¸ä»£ç ']:
                if col in df.columns:
                    code_col = col
                    break
            
            # å¯»æ‰¾åç§°åˆ— (Eastmoney ç‰¹æœ‰çš„å¯èƒ½æ˜¯ board_name æˆ– concept_name)
            name_col = None
            for col in ['board_name', 'concept_name', 'åç§°', 'æ¿å—åç§°']:
                if col in df.columns:
                    name_col = col
                    break
            
            if not code_col or not name_col:
                logging.warning(f"Eastmoney {label}æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {file_path}")
                return

            count = 0
            for _, row in df.iterrows():
                val = row[code_col]
                if pd.isna(val): continue
                
                code = str(val).zfill(6)
                name = str(row[name_col]).strip()
                
                if name == 'nan' or not name: continue
                
                if code not in target_map:
                    target_map[code] = []
                if name not in target_map[code]:
                    target_map[code].append(name)
                    count += 1
            logging.info(f"âœ… Eastmoney {label}æ˜ å°„åŠ è½½å®Œæˆ, å…±åŠ è½½ {count} æ¡æ˜ å°„, è¦†ç›– {len(target_map)} åªè‚¡ç¥¨")
        except Exception as e:
            logging.warning(f"åŠ è½½ Eastmoney {label}æ˜ å°„å¤±è´¥: {e}")
    
    def load_industry_mapping(self, mapping_file: str):
        """
        åŠ è½½è¡Œä¸šæ˜ å°„å…³ç³»(å…¼å®¹æ—§æ¥å£)
        
        Args:
            mapping_file: æ˜ å°„æ–‡ä»¶è·¯å¾„(CSVæ ¼å¼,åŒ…å«stock_codeå’Œindustryåˆ—)
        """
        try:
            df = pd.read_csv(mapping_file)
            for _, row in df.iterrows():
                code = str(row['stock_code']).zfill(6)
                industry = row['industry']
                if code not in self.industry_map:
                    self.industry_map[code] = []
                if industry not in self.industry_map[code]:
                    self.industry_map[code].append(industry)
            logging.info(f"è¡Œä¸šæ˜ å°„åŠ è½½å®Œæˆ,å…± {len(self.industry_map)} æ¡è®°å½•")
        except Exception as e:
            logging.error(f"åŠ è½½è¡Œä¸šæ˜ å°„å¤±è´¥: {e}")
    
    def load_stock_names(self):
        """åŠ è½½è‚¡ç¥¨åç§°æ˜ å°„"""
        try:
            # ä»parquetæ–‡ä»¶ä¸­æå–è‚¡ç¥¨åç§°
            for stock_code, df in self.all_data.items():
                if 'stock_name' in df.columns and len(df) > 0:
                    self.stock_name_map[stock_code] = df['stock_name'].iloc[0]
            
            # å¦‚æœæœ‰CSVæ–‡ä»¶ï¼Œä¹Ÿä»CSVåŠ è½½
            import os
            csv_path = 'data/eastmoney_all_stocks.csv'
            if os.path.exists(csv_path):
                import pandas as pd
                df = pd.read_csv(csv_path)
                df['stock_code'] = df['stock_code'].astype(str).str.zfill(6)
                name_dict = dict(zip(df['stock_code'], df['stock_name']))
                self.stock_name_map.update(name_dict)
                
            logging.info(f"è‚¡ç¥¨åç§°æ˜ å°„åŠ è½½å®Œæˆï¼Œå…± {len(self.stock_name_map)} æ¡è®°å½•")
        except Exception as e:
            logging.warning(f"åŠ è½½è‚¡ç¥¨åç§°æ˜ å°„å¤±è´¥: {e}")
    
    def get_stock_name(self, stock_code: str) -> str:
        """è·å–è‚¡ç¥¨åç§°"""
        return self.stock_name_map.get(stock_code, stock_code)
    
    def load_pre_close_prices(self):
        """åŠ è½½è‚¡ç¥¨æ˜¨æ”¶ä»·"""
        try:
            import os
            # æ”¯æŒä¸¤ç§æ•°æ®ç»“æ„:
            # 1. data/20251222/tick/*.parquet + data/20251222/stock_pre_close_20251222.csv
            # 2. data/tick_20251222/*.parquet + data/stock_pre_close_20251222.csv
            
            # ä»æ•°æ®ç›®å½•åæå–æ—¥æœŸ
            dir_name = self.data_dir.name
            
            # å°è¯•æ–°ç»“æ„ (data/20251222/tick)
            if dir_name == 'tick':
                date_str = self.data_dir.parent.name
                pre_close_file = self.data_dir.parent / f'stock_pre_close_{date_str}.csv'
            # å°è¯•æ—§ç»“æ„ (data/tick_20251222)
            elif dir_name.startswith('tick_'):
                date_str = dir_name.replace("tick_", "")
                pre_close_file = Path(f'data/stock_pre_close_{date_str}.csv')
            else:
                # ç›´æ¥ä½¿ç”¨ç›®å½•åä½œä¸ºæ—¥æœŸ (data/20251222)
                date_str = dir_name
                pre_close_file = self.data_dir / f'stock_pre_close_{date_str}.csv'
            
            if os.path.exists(pre_close_file):
                df = pd.read_csv(pre_close_file)
                df['stock_code'] = df['stock_code'].astype(str).str.zfill(6)
                # æ˜¨æ”¶ä»·å•ä½å¯èƒ½æ˜¯åˆ†ï¼Œéœ€è¦è½¬æ¢ä¸ºå…ƒ
                df['pre_close'] = df['pre_close'] / 100
                self.pre_close_map = dict(zip(df['stock_code'], df['pre_close']))
                logging.info(f"æ˜¨æ”¶ä»·åŠ è½½å®Œæˆï¼Œå…± {len(self.pre_close_map)} æ¡è®°å½• (æ–‡ä»¶: {pre_close_file})")
            else:
                logging.warning(f"æœªæ‰¾åˆ°æ˜¨æ”¶ä»·æ–‡ä»¶: {pre_close_file}")
        except Exception as e:
            logging.warning(f"åŠ è½½æ˜¨æ”¶ä»·å¤±è´¥: {e}")
    
    def replay_iterator(self, start_time: str = "09:30:00", 
                       end_time: str = "15:00:00",
                       speed_seconds: int = 1):
        """
        ç”Ÿæˆå™¨: æŒ‰æ—¶é—´é¡ºåºå›æ”¾æ•°æ®
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            speed_seconds: å›æ”¾é€Ÿåº¦(ç§’/æ¬¡)
            
        Yields:
            (current_time, snapshot)
        """
        # è§£ææ—¶é—´
        start_dt = pd.to_datetime(f"2025-01-01 {start_time}")
        end_dt = pd.to_datetime(f"2025-01-01 {end_time}")
        
        current = start_dt
        
        while current <= end_dt:
            self.current_time = current
            
            # è·å–å¿«ç…§
            snapshot = self.get_snapshot_at_time(current)
            
            yield current, snapshot
            
            # å¢åŠ æ—¶é—´æ­¥é•¿
            current += pd.Timedelta(seconds=speed_seconds)


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    engine = ReplayEngine("data/tick_20251216")
    
    # æµ‹è¯•åŠ è½½
    logging.info("æ­£åœ¨åŠ è½½æ•°æ®...")
    # engine.load_all_data()
    
    logging.info("å¤ç›˜å¼•æ“åˆå§‹åŒ–å®Œæˆ")
